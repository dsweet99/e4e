{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5\n",
    "# Contextual bandits: Make targeted decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from e4e import E4E\n",
    "\n",
    "e4e = E4E(chapter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1\tModel a business metric offline to make decisions online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1\tDefine the prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMULATE THE VIEWING TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.1 Simulate viewing time\n",
    "def measure_viewing_time(context, action_weights):\n",
    "    return np.exp( (context*action_weights).mean() + 0.1*np.random.normal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "action_weights = np.random.normal(size=(5,))\n",
    "m = [measure_viewing_time(context=np.random.normal(size=(5,)), action_weights=action_weights) \n",
    "     for _ in range(1000)]\n",
    "plt.hist(m,25, color=e4e.color_1);\n",
    "plt.xlabel('viewing time')\n",
    "plt.ylabel('counts')\n",
    "e4e.save_fig(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIT THE PREDICTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.2 A logged sample\n",
    "class Sample:\n",
    "    def __init__(self, context, action, reward):\n",
    "        self.context = context\n",
    "        self.action = action\n",
    "        self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.3 Collect logs for each action\n",
    "def collect_logs_by_action(num_actions, logs):\n",
    "    samples_y = [[] for _ in range(num_actions)]\n",
    "    samples_x = [[] for _ in range(num_actions)]\n",
    "    for sample in logs:\n",
    "        samples_y[sample.action].append(sample.reward)\n",
    "        samples_x[sample.action].append(sample.context)\n",
    "    return samples_y, samples_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.4 Build a model for each action\n",
    "def build_models(num_features, samples_y, samples_x):\n",
    "    betas = []\n",
    "    ngood=0\n",
    "    nbad=0\n",
    "    for y, x in zip(samples_y, samples_x): # for each action\n",
    "        y = np.array(y)\n",
    "        x = np.array(x)\n",
    "        if len(y) > 0:\n",
    "            beta = np.linalg.pinv(x.T@x) @ x.T@y\n",
    "            ngood+=1\n",
    "        else:\n",
    "            beta = np.zeros(shape=(num_features,))\n",
    "            nbad+=1\n",
    "        betas.append(beta)\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2\tAdd the decision-making component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.5 A greedy recommender\n",
    "class RecommenderGreedy:\n",
    "    def __init__(self, num_features, num_actions):\n",
    "        self._num_features = num_features\n",
    "        self._num_actions = num_actions\n",
    "        \n",
    "    def reset(self):\n",
    "        self._betas = [np.random.normal(size=(num_features, )) for _ in range(self._num_actions)]\n",
    "        \n",
    "    def fit_offline(self, logs):\n",
    "        samples_y, samples_x = collect_logs_by_action(num_actions, logs)\n",
    "        self._betas = build_models(self._num_features, samples_y, samples_x)\n",
    "        \n",
    "    def policy(self, context):\n",
    "        viewing_max = -np.inf\n",
    "        for action in range(self._num_actions):\n",
    "            viewing_hat = context @ self._betas[action]\n",
    "            if viewing_hat > viewing_max:\n",
    "                action_best = action\n",
    "                viewing_max = viewing_hat\n",
    "        return action_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3\tRun and evaluate the greedy recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_production_data(action_weights, recommender):\n",
    "    logs = []\n",
    "    total_viewing_time = 0\n",
    "    num_decisions = 100\n",
    "    for _ in range(num_decisions):\n",
    "        context = np.random.randint(2, size=(len(action_weights),)) # features describing user\n",
    "        context[0] = 1 # first \"feature\" is just a constant / intercept term / offset\n",
    "        action = recommender.policy(context) # choose best post\n",
    "        viewing_time = measure_viewing_time(context=context, action_weights=action_weights[:, action])\n",
    "        logs.append( Sample(context, action, viewing_time) )\n",
    "        total_viewing_time += viewing_time\n",
    "    avg_viewing_time = (total_viewing_time)/num_decisions\n",
    "    return avg_viewing_time, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_sequence(action_weights, num_actions, recommender):\n",
    "    num_days = 14\n",
    "    num_features = action_weights.shape[0]\n",
    "    avg_viewing_times = []\n",
    "    all_logs = []\n",
    "    recommender.reset()\n",
    "    for _ in range(num_days):  # one month\n",
    "        avg_viewing_time, logs = log_production_data(action_weights, recommender)\n",
    "        avg_viewing_times.append(avg_viewing_time)\n",
    "        all_logs.extend(logs)\n",
    "        recommender.fit_offline(all_logs)  # all data from day one till now\n",
    "\n",
    "    avg_viewing_times = np.array(avg_viewing_times)\n",
    "    return avg_viewing_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequences(action_weights, num_actions, recommender):\n",
    "    avg_viewing_times = []\n",
    "    num_runs = 10\n",
    "    for _ in range(num_runs):\n",
    "        avg_viewing_times.append(run_experiment_sequence(action_weights, num_actions, recommender))\n",
    "    avg_viewing_times = np.array(avg_viewing_times)\n",
    "    mean = avg_viewing_times.mean(axis=0)\n",
    "    se = avg_viewing_times.std(axis=0)/np.sqrt(num_runs)\n",
    "    return mean, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 5\n",
    "num_actions = 30\n",
    "\n",
    "np.random.seed(17)\n",
    "action_weights = np.random.normal(size=(num_features, num_actions)) # the dgp; fixed values\n",
    "recommender = RecommenderGreedy(num_features, num_actions)\n",
    "mean, se = run_sequences(action_weights, num_actions, recommender)\n",
    "betas_g = recommender._betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean, '.-', color=e4e.color_1)\n",
    "plt.fill_between(np.arange(len(mean)),\n",
    "                 mean - se,\n",
    "                 mean + se,\n",
    "                 color=e4e.color_2, alpha=e4e.alpha_err, linewidth=1);\n",
    "\n",
    "asymp = mean[4:].mean()\n",
    "e4e.horizontal_line(asymp, e4e.color_1)\n",
    "\n",
    "plt.annotate(f\"asymptote, mean viewing time = {asymp:.2f}\", xy=[0, asymp-.01],\n",
    "             xytext=[2, 1.4],\n",
    "             arrowprops=e4e.arrow_props\n",
    "            )\n",
    "\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('mean viewing time')\n",
    "e4e.save_fig(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2\tExplore actions with epsilon-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1\tMissing counterfactuals degrade predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = 1, no missing data\n",
    "contexts = [\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "rewards = [\n",
    "    .6,\n",
    "    .9,\n",
    "    1.3\n",
    "]\n",
    "x = np.array(contexts)\n",
    "y = np.array(rewards)\n",
    "beta_1 = np.linalg.pinv(x.T @ x) @ (x.T@y)\n",
    "print (beta_1)\n",
    "\n",
    "# predicted response of user a to action=1\n",
    "context_a = [0,0,1]\n",
    "print (context_a @ beta_1)\n",
    "\n",
    "# predicted response of user b to action=1\n",
    "context_b = [1,0,1]\n",
    "print (context_b @ beta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = 1, missing data about feature #3\n",
    "contexts = [\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0]\n",
    "]\n",
    "rewards = [\n",
    "    0.6,\n",
    "    0.9\n",
    "]\n",
    "x = np.array(contexts)\n",
    "y = np.array(rewards)\n",
    "beta_1m = np.linalg.pinv(x.T @ x) @ (x.T@y)\n",
    "print (beta_1m)\n",
    "\n",
    "# predicted response of user a to action=1\n",
    "print (context_a @ beta_1m)\n",
    "\n",
    "# predicted response of user b to action=1\n",
    "print (context_b @ beta_1m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEEDBACK LOOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2\tExplore with epsilon-greedy to collect counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.6 Epsilon-greedy recommender\n",
    "class RecommenderEpsilonGreedy:\n",
    "    def __init__(self, num_features, num_actions, eps=0.1):\n",
    "        self._num_features = num_features\n",
    "        self._num_actions = num_actions\n",
    "        self._eps = eps\n",
    "        \n",
    "    def reset(self):\n",
    "        self._betas = [np.random.normal(size=(num_features, )) for _ in range(self._num_actions)]\n",
    "        \n",
    "    def fit_offline(self, logs):\n",
    "        samples_y, samples_x = collect_logs_by_action(num_actions, logs)\n",
    "        self._betas = build_models(self._num_features, samples_y, samples_x)\n",
    "        \n",
    "    def policy(self, context):\n",
    "        viewing_max = -np.inf\n",
    "        if np.random.uniform(0,1) < self._eps:\n",
    "            action_best = np.random.randint(0, self._num_actions)\n",
    "        else:\n",
    "            for action in range(self._num_actions):\n",
    "                viewing_hat = context @ self._betas[action]\n",
    "                if viewing_hat > viewing_max:\n",
    "                    action_best = action\n",
    "                    viewing_max = viewing_hat\n",
    "        return action_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "recommender = RecommenderEpsilonGreedy(num_features, num_actions, eps=0.1)\n",
    "mean_eps, se_eps = run_sequences(action_weights, num_actions, recommender)\n",
    "betas_eg = recommender._betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean, '.-', color=e4e.color_1)\n",
    "plt.plot(mean_eps, '.--', color=e4e.color_1)\n",
    "\n",
    "plt.fill_between(np.arange(len(mean)),\n",
    "                 mean - se,\n",
    "                 mean + se,\n",
    "                 color=e4e.color_2, alpha=e4e.alpha_err, linewidth=1);\n",
    "\n",
    "plt.fill_between(np.arange(len(mean_eps)),\n",
    "                 mean_eps - se_eps,\n",
    "                 mean_eps + se_eps,\n",
    "                 color=e4e.color_2, alpha=e4e.alpha_err, linewidth=1);\n",
    "\n",
    "plt.legend(['RecommenderGreedy', 'RecommenderEpsilonGreedy'])\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('mean viewing time')\n",
    "e4e.save_fig(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Explore parameters by Thompson sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1\tCreate an ensemble of prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "visits = np.array([3 + int(5*np.random.uniform()) for _ in range(100)])\n",
    "i = np.random.randint(len(visits), size=(len(visits,)))\n",
    "bs_visits = visits[i]\n",
    "print (visits.mean(), visits.std())\n",
    "print (bs_visits.mean(), bs_visits.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.hist(visits, 25, color=e4e.color_1);\n",
    "e4e.aspect_square(ax1)\n",
    "ax1.set_title('Measured sample set')\n",
    "\n",
    "ax2.hist(bs_visits, 25, color=e4e.color_1);\n",
    "e4e.aspect_square(ax2)\n",
    "ax2.set_title('Bootstrap sample set')\n",
    "\n",
    "e4e.save_fig(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.7 Thompson sampling recommender\n",
    "class RecommenderThompsonSampling:\n",
    "    def __init__(\n",
    "        self, num_features, num_actions,\n",
    "        num_bs_samples\n",
    "    ):\n",
    "        self._num_features = num_features\n",
    "        self._num_actions = num_actions\n",
    "        self._num_bs_samples = num_bs_samples\n",
    "        \n",
    "    def reset(self):\n",
    "        self._betas = []\n",
    "        for _ in range(self._num_bs_samples):\n",
    "            self._betas.append([\n",
    "                np.random.normal(size=(num_features,))\n",
    "                for _ in range(self._num_actions)\n",
    "            ] )\n",
    "        \n",
    "    def _bs_sample(self, samples_y, samples_x):\n",
    "        bs_samples_y = []\n",
    "        bs_samples_x = []\n",
    "        for action in range(self._num_actions):\n",
    "            y = np.array(samples_y[action])\n",
    "            x = np.array(samples_x[action])\n",
    "            if len(y)>0:\n",
    "                i = np.random.randint(0, len(y), size=(len(y),))\n",
    "                y = y[i]\n",
    "                x = x[i,:]\n",
    "            bs_samples_y.append(y)\n",
    "            bs_samples_x.append(x)\n",
    "        return bs_samples_y, bs_samples_x\n",
    "        \n",
    "    def fit_offline(self, logs):\n",
    "        fit_logs = logs\n",
    "        samples_y, samples_x = collect_logs_by_action(\n",
    "            num_actions, fit_logs\n",
    "        )\n",
    "        self._betas = []\n",
    "        for _ in range(self._num_bs_samples):\n",
    "            bs_samples_y, bs_samples_x = self._bs_sample(\n",
    "                samples_y, samples_x\n",
    "            )\n",
    "            self._betas.append(build_models(\n",
    "                self._num_features, bs_samples_y, bs_samples_x\n",
    "            ))\n",
    "        \n",
    "    def policy(self, context):\n",
    "        i_beta = np.random.randint(0, self._num_bs_samples)\n",
    "        beta = self._betas[i_beta]\n",
    "        viewing_max = -np.inf\n",
    "        for action in range(self._num_actions):\n",
    "            viewing_hat = context @ beta[action]\n",
    "            if viewing_hat > viewing_max:\n",
    "                action_best = action\n",
    "                viewing_max = viewing_hat\n",
    "        return action_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "recommender = RecommenderThompsonSampling(num_features, num_actions, num_bs_samples=30)\n",
    "mean_ts, se_ts = run_sequences(action_weights, num_actions, recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean, '.-', color=e4e.color_1)\n",
    "plt.plot(mean_eps, '.--', color=e4e.color_1)\n",
    "plt.plot(mean_ts, ':.', color=e4e.color_1)\n",
    "\n",
    "plt.fill_between(np.arange(len(mean)),\n",
    "                 mean - se,\n",
    "                 mean + se,\n",
    "                 color=e4e.color_2, alpha=e4e.alpha_err, linewidth=1);\n",
    "\n",
    "plt.fill_between(np.arange(len(mean_eps)),\n",
    "                 mean_eps - se_eps,\n",
    "                 mean_eps + se_eps,\n",
    "                 color=e4e.color_2, alpha=e4e.alpha_err, linewidth=1);\n",
    "\n",
    "plt.fill_between(np.arange(len(mean_eps)),\n",
    "                 mean_ts - se_ts,\n",
    "                 mean_ts + se_ts,\n",
    "                 color=e4e.color_2, alpha=e4e.alpha_err, linewidth=1);\n",
    "\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('mean viewing time')\n",
    "plt.legend(['RecommenderGreedy', 'RecommenderEpsilonGreedy', 'RecommenderThompsonSampling'])\n",
    "e4e.save_fig(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2\tRandomized probability matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderThompsonSamplingInstrumented:\n",
    "    def __init__(\n",
    "        self, num_features, num_actions,\n",
    "        num_bs_samples\n",
    "    ):\n",
    "        self._num_features = num_features\n",
    "        self._num_actions = num_actions\n",
    "        self._num_bs_samples = num_bs_samples\n",
    "        \n",
    "    def reset(self):\n",
    "        self._betas = []\n",
    "        for _ in range(self._num_bs_samples):\n",
    "            self._betas.append([\n",
    "                np.random.normal(size=(num_features,))\n",
    "                for _ in range(self._num_actions)\n",
    "            ] )\n",
    "        self._p_best = []\n",
    "        self.mean_vs_day = []\n",
    "        \n",
    "    def _bs_sample(self, samples_y, samples_x):\n",
    "        bs_samples_y = []\n",
    "        bs_samples_x = []\n",
    "        for action in range(self._num_actions):\n",
    "            y = np.array(samples_y[action])\n",
    "            x = np.array(samples_x[action])\n",
    "            if len(y)>0:\n",
    "                i = np.random.randint(0, len(y), size=(len(y),))\n",
    "                y = y[i]\n",
    "                x = x[i,:]\n",
    "            bs_samples_y.append(y)\n",
    "            bs_samples_x.append(x)\n",
    "        return bs_samples_y, bs_samples_x\n",
    "        \n",
    "    def fit_offline(self, logs):\n",
    "        self.mean_vs_day.append(np.array(self._p_best).mean())\n",
    "            \n",
    "        fit_logs = logs\n",
    "        samples_y, samples_x = collect_logs_by_action(\n",
    "            num_actions, fit_logs\n",
    "        )\n",
    "        self._betas = []\n",
    "        for _ in range(self._num_bs_samples):\n",
    "            bs_samples_y, bs_samples_x = self._bs_sample(\n",
    "                samples_y, samples_x\n",
    "            )\n",
    "            self._betas.append(build_models(\n",
    "                self._num_features, bs_samples_y, bs_samples_x\n",
    "            ))\n",
    "        \n",
    "    def _best_post(self, context, beta):\n",
    "        viewing_max = -np.inf\n",
    "        for action in range(self._num_actions):\n",
    "            viewing_hat = context @ beta[action]\n",
    "            if viewing_hat > viewing_max:\n",
    "                action_best = action\n",
    "                viewing_max = viewing_hat\n",
    "        return action_best        \n",
    "        \n",
    "    def policy(self, context):\n",
    "        best_posts = [\n",
    "            self._best_post(context, self._betas[i_beta])\n",
    "            for i_beta in range(self._num_bs_samples)\n",
    "        ]\n",
    "    \n",
    "        i_beta = np.random.randint(self._num_bs_samples)\n",
    "        action_best = best_posts[i_beta]\n",
    "        num = 0\n",
    "        for bp in best_posts:\n",
    "            if bp == action_best:\n",
    "                num += 1\n",
    "            \n",
    "        p_post = num / self._num_bs_samples\n",
    "        self._p_best.append(p_post)\n",
    "        return action_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "recommender = RecommenderThompsonSamplingInstrumented(num_features, num_actions, num_bs_samples=30)\n",
    "run_sequences(action_weights, num_actions, recommender);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array(recommender.mean_vs_day)\n",
    "plt.plot(m,'.--', color=e4e.color_1)\n",
    "\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('avg. $p_{best}(action)$')\n",
    "e4e.save_fig(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
